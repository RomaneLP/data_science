{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b835fee3-52c9-4013-83c6-32a33e518f16",
   "metadata": {},
   "source": [
    "# Projet Python pour la data science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd028b1-8ba9-43b8-8694-780da68c94ac",
   "metadata": {},
   "source": [
    "### Auteurs: Arthur LEROUDIER, Romane LE POTIER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa20487-31c8-4abe-ae30-c15f69cdc01a",
   "metadata": {},
   "source": [
    "##### L'objectif de ce notebook est de présenter le projet que nous avons effectué dans le cadre de l'unité d'enseignement Python pour la data science à l'ENSAE. Ce projet contient une première partie sur la récupération d'une base de données par web scraping puis une seconde partie nettoyage de la base de données et visualisation et une dernière partie sur la modélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebcc8df-3b18-4391-9aaa-7b13654d0841",
   "metadata": {},
   "source": [
    "# Quelle est l'importance d'un joueur sur l'issu d'un match de basket ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9658123-cd29-48ad-b56e-4c40ac753c39",
   "metadata": {},
   "source": [
    "##### Pour répondre à cette problématique, nous allons étudier les statistiques de joueurs sur 10 matchs de basket, puis prendre la moyenne des statistiques d'un joueur sur la saison et modéliser la probabilité de gagner le match quand ce joueur est sur le terrain. Dans notre cas, on a choisi Victor Wembanyama qui est un joueur français qui fait sa 1ère saison en NBA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce66f4c9-f6e2-4735-80d6-2a3ce43944b5",
   "metadata": {},
   "source": [
    "## Installation préalable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4636067-e7da-4d39-8b77-8398874122b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.10/site-packages (from webdriver-manager) (2.31.0)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in /opt/mamba/lib/python3.10/site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests->webdriver-manager) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests->webdriver-manager) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests->webdriver-manager) (2023.11.17)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.0 webdriver-manager-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q lxml\n",
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9ad0e-0559-409b-be46-4e0a22fbc7bc",
   "metadata": {},
   "source": [
    "## Importation des modules utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57e9703b-f719-4ba7-8159-a45f20c29cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import lxml\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b725d40-3449-4da2-8c69-7ecd110951e3",
   "metadata": {},
   "source": [
    "# Récupération et traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6e594-4dba-489f-8f47-7a3adeeaaa7b",
   "metadata": {},
   "source": [
    "## Extraction des données de 10 matchs de basket par web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca2238-7b5f-44d7-968c-5f78b4d0c553",
   "metadata": {},
   "source": [
    "##### On a décidé d'extraire les tableaux de données sur le site basketball reference des 10 matchs qui ont eu lieu le 25 octobre 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87964275-9392-49a7-83b4-26b4eb8786b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# à changer pour chaque match avec 1 qui veut dire match gagné et 0 match perdu\n",
    "url_match=\"http://bkref.com/pi/shareit/WumlT\"\n",
    "titre = 'Orlando'\n",
    "result = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8bbd31-6623-4a28-9096-595ed743baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       MP FG FGA    FG% 3P 3PA   3P% FT FTA    FT%  ... DRB  \\\n",
      "Paolo Banchero      27:54  3   6   .500  0   1  .000  6   7   .857  ...   5   \n",
      "Franz Wagner        27:03  5  12   .417  3   7  .429  6   6  1.000  ...   1   \n",
      "Wendell Carter Jr.  26:55  4  10   .400  0   2  .000  0   0         ...   4   \n",
      "Markelle Fultz      24:31  5   9   .556  0   0        0   0         ...   1   \n",
      "Jalen Suggs         20:51  3  12   .250  2   7  .286  0   0         ...   3   \n",
      "Cole Anthony        24:05  8  12   .667  1   2  .500  3   3  1.000  ...   7   \n",
      "Joe Ingles          18:14  0   3   .000  0   3  .000  0   0         ...   4   \n",
      "Gary Harris         16:57  5   8   .625  1   4  .250  0   0         ...   2   \n",
      "Moritz Wagner       15:21  0   1   .000  0   1  .000  2   2  1.000  ...   4   \n",
      "Jonathan Isaac      14:09  4   6   .667  1   3  .333  2   3   .667  ...   3   \n",
      "Goga Bitadze         4:48  2   2  1.000  0   0        3   4   .750  ...   2   \n",
      "Anthony Black        4:48  2   2  1.000  0   0        1   1  1.000  ...   0   \n",
      "Caleb Houstan        4:48  1   2   .500  1   2  .500  0   0         ...   1   \n",
      "Jett Howard          4:48  0   1   .000  0   1  .000  0   0         ...   0   \n",
      "Chuma Okeke          4:48  0   1   .000  0   1  .000  0   0         ...   3   \n",
      "\n",
      "                   TRB AST STL BLK TOV PF PTS  +/- Résultat  \n",
      "Paolo Banchero       5   5   1   0   1  2  12  +13        1  \n",
      "Franz Wagner         4   2   1   0   3  1  19  +10        1  \n",
      "Wendell Carter Jr.   8   1   2   0   2  3   8   +8        1  \n",
      "Markelle Fultz       4   2   3   0   1  0  10  +12        1  \n",
      "Jalen Suggs          4   0   0   0   0  2   8   +5        1  \n",
      "Cole Anthony         8   2   0   0   0  2  20  +17        1  \n",
      "Joe Ingles           4   5   0   0   1  3   0   +9        1  \n",
      "Gary Harris          5   0   0   1   0  0  11   +8        1  \n",
      "Moritz Wagner        4   0   0   1   3  4   2  +13        1  \n",
      "Jonathan Isaac       4   0   0   2   2  2  11  +10        1  \n",
      "Goga Bitadze         3   0   1   0   1  1   7   +9        1  \n",
      "Anthony Black        0   1   0   2   0  0   5   +9        1  \n",
      "Caleb Houstan        1   0   0   0   0  0   3   +9        1  \n",
      "Jett Howard          0   1   0   0   0  0   0   +9        1  \n",
      "Chuma Okeke          3   0   0   0   0  0   0   +9        1  \n",
      "\n",
      "[15 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "request_text = request.urlopen(url_match).read()\n",
    "#print(request_text[:1000])\n",
    "page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
    "#print(page)\n",
    "\n",
    "tableau_match = page.find('table', {'class' : 'sortable stats_table now_sortable modifying'})\n",
    "#print(tableau_match)\n",
    "\n",
    "table_body = tableau_match.find('tbody')\n",
    "rows = table_body.find_all('tr')\n",
    "#print(rows[1])\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    #print(cols)\n",
    "\n",
    "dico_participants = dict()\n",
    "for row in rows:\n",
    "    nom = row.find_all('th')\n",
    "    nom = [ele.text.strip() for ele in nom]\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    cols = nom + cols\n",
    "    if len(cols) > 0 : \n",
    "        dico_participants[cols[0]] = cols[1:]\n",
    "#dico_participants\n",
    "\n",
    "table_head = tableau_match.find('thead')\n",
    "titres_col = table_head.find_all('th')\n",
    "titres_col = [ele.text.strip() for ele in titres_col[3:]]\n",
    "#print(titres_col)\n",
    "\n",
    "data_participants = pd.DataFrame.from_dict(dico_participants,orient='index', columns = titres_col)\n",
    "data_participants['Résultat'] = [result for index in range(data_participants.shape[0])]\n",
    "data_participants.to_csv('data/'+titre+'_Stats.csv')\n",
    "print(data_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b5929-3a03-4b34-95e9-0181596cd339",
   "metadata": {},
   "source": [
    "##### On a donc effectué ce code 20 fois pour les 20 équipes différentes en rajoutant aux données la dernière colonne résultat. Ces 20 tableaux se trouvent dans le dossier data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf890f-5267-4b1d-b0d0-b033f2734a59",
   "metadata": {},
   "source": [
    "##### Pour chaque tableau, on a à gauche le nom des joueurs et pour les colonnes:\n",
    "##### MP: temps joué dans le match\n",
    "##### FG: tirs marqués à 2 points\n",
    "##### FGA: tirs tentés à 2 points\n",
    "##### FG%: pourcentage de réussite à 2 points\n",
    "##### 3P: tirs marqués à 3 points\n",
    "##### 3PA: tirs tentés à 3 points\n",
    "##### 3P%: pourcentage de réussite à 3 points\n",
    "##### FT: lancers francs marqués\n",
    "##### FTA: lancers francs tentés\n",
    "##### FT%: pourcentage de lancers francs réussis\n",
    "##### ORB: nombre de rebonds offensifs\n",
    "##### DRB: nombre de rebonds défensifs\n",
    "##### TRB: nombre de rebonds totaux\n",
    "##### AST: nombre de passs décisives\n",
    "##### STL: nombre d'interceptions\n",
    "##### BLK: nombre de contres\n",
    "##### TOV: nombre de balles perdues\n",
    "##### PF: nombre de fautes commises\n",
    "##### PTS: nombre de points marqués\n",
    "##### +/-: différence de points marqués entre les deux équipes quand le joueur  est sur le terrain\n",
    "##### Résultat: 1 si match gagné, 0 sinon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539676a-1cd4-4f48-b77c-e5c91f58f708",
   "metadata": {},
   "source": [
    "# Nettoyage de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f079173-a07e-4c2e-b16f-f7695cc43571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#commande à n'exécuter qu'une fois\n",
    "os.chdir('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3852b-ff81-414a-8490-d27a97637091",
   "metadata": {},
   "source": [
    "### On commence par concaténer nos 20 tableaux en un seul tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97312b3-bbd4-4162-bb6f-f4e58aa69d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'individus:  223\n"
     ]
    }
   ],
   "source": [
    "#on prend la liste des fichiers\n",
    "files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "data = pd.read_csv(files[0])\n",
    "#la colonne +/- n'est pas prise en compte car on a pas sa moyenne pour les matchs de Wembanyama\n",
    "data = data.loc[:, data.columns!='+/-']\n",
    "#conversion des minutes par match en flottant\n",
    "data.loc[:,'MP'] = [float(x.split(':')[0])+ 0.006*float(x.split(':')[1]) for x in data.loc[:,'MP']]\n",
    "#concatenation de toutes les tables csv en un seul tableau\n",
    "for file in files[1:]:\n",
    "    match = pd.read_csv(file)\n",
    "    match = match.loc[:,match.columns!='+/-']\n",
    "    match.loc[:,'MP'] = [float(x.split(':')[0])+ 0.006*float(x.split(':')[1]) for x in match.loc[:,'MP']]\n",
    "    data = pd.concat([data,match])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"nombre d'individus: \", len(data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf7d7e-d3a9-40ea-98d8-92540a833062",
   "metadata": {},
   "source": [
    "##### On a donc les données d'un match pour 223 joueurs différents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d636a79-ac40-4831-8abd-3f58ce6657b8",
   "metadata": {},
   "source": [
    "##### Pour certains joueurs, des informations manquent et des cases du tableau sont donc vides. On remplace ces cases vides par une donnée de la même colonne pour des joueurs ayant la même valeur dans la colonne résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf67920a-bd4f-498a-9144-4fdfbe6828e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation des données manquantes\n",
    "#remplace les données absentes par une donnée de la même colonne, avec la même valeur pour la variable Résultat\n",
    "data_win = data[data['Résultat'] == 1]\n",
    "data_lose = data[data['Résultat'] != 1]\n",
    "for col in data_win.columns:\n",
    "    for i,rows in data_win.iterrows():\n",
    "        if pd.isna(data_win.loc[i,col]):\n",
    "            data_win.loc[i,col] = np.random.choice(data_win[data_win[col].notna()].loc[:,col])\n",
    "for col in data_lose.columns:\n",
    "    for i,rows in data_lose.iterrows():\n",
    "        if pd.isna(data_lose.loc[i,col]):\n",
    "            data_lose.loc[i,col] = np.random.choice(data_lose[data_lose[col].notna()].loc[:,col])\n",
    "data = pd.concat([data_win,data_lose])\n",
    "\n",
    "X_train = data.loc[:, data.columns!='Résultat']\n",
    "X_train = X_train.iloc[:,1:]\n",
    "Y = data.loc[:,'Résultat']\n",
    "VW = np.array([30.2 ,6.9 ,15.9 ,0.435 ,1.4 ,4.9 ,0.278 ,3.3 ,4.2 ,0.774 ,2.3 ,8.4 ,10.7 ,2.8 ,1.4 ,3.0 ,3.3 ,2.5 ,18.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c5fc6-2c18-449a-ac76-64027fce10d6",
   "metadata": {},
   "source": [
    "# Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db20c9e-8ac3-4472-9a9d-3b09943d02ae",
   "metadata": {},
   "source": [
    "##### On commence par normer et centrer les variables afin de pouvoir réaliser une régression linéaire par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49e58760-0676-46b8-b36f-c3dc9ac56ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardisation\n",
    "scaler = sk.preprocessing.StandardScaler().fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f6e36a5-f09b-478a-b1d0-f28960b9558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef de  Unnamed: 0 : -0.10200421522531417\n",
      "coef de  MP : 0.03021895237398286\n",
      "coef de  FG : -0.06379910922837341\n",
      "coef de  FGA : 0.005506828738304873\n",
      "coef de  FG% : 0.18345760804905095\n",
      "coef de  3P : -0.09530994272002209\n",
      "coef de  3PA : -0.1023533005264111\n",
      "coef de  3P% : -0.05642665356313856\n",
      "coef de  FT : 0.03806785926906024\n",
      "coef de  FTA : 0.16229592936445797\n",
      "coef de  FT% : -0.016127075200177803\n",
      "coef de  ORB : 0.03587919674640999\n",
      "coef de  DRB : 0.02084757255931242\n",
      "coef de  TRB : 0.028133664249904093\n",
      "coef de  AST : 0.012168011040625641\n",
      "coef de  STL : 0.00929187981933716\n",
      "coef de  BLK : -0.012621348319038135\n",
      "coef de  TOV : 0.02723027554497316\n",
      "coef de  PF : 0.03998411397410368\n",
      "constante:  0.5112107623318385\n"
     ]
    }
   ],
   "source": [
    "#régression linéaire\n",
    "reg = LinearRegression().fit(X_scaled, Y)\n",
    "for k in range(len(reg.coef_)):\n",
    "    print('coef de ',data.columns[k], ':', reg.coef_[k])\n",
    "print('constante: ',reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7a72d-9c98-43de-90a3-19326d98a3b9",
   "metadata": {},
   "source": [
    "##### Ainsi, on obtient les coefficients des variables. Cependant, on peut remarquer que les résultats sont peu pertinents. Cela est dû au fait que la quantité à prédire est un booléen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24edd048-ca63-4563-89fd-12eae07cf503",
   "metadata": {},
   "source": [
    "##### Il paraît donc plus judicieux de réaliser une régression logistique puisqu'un réseau de neurone à 1 neurone fonctionne comme une régression linéaire mais prédit un booléen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d67aa2f-cdbf-49ab-875b-35a8ca23dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "prédiction:  [0]\n",
      "proba de perdre:  0.9999675979580431\n",
      "proba de gagner:  3.2402041956826914e-05\n"
     ]
    }
   ],
   "source": [
    "#régression logistique\n",
    "clf = LogisticRegression(random_state=0).fit(X_scaled, Y)\n",
    "VW = VW.reshape(1, -1)\n",
    "print(np.shape(VW))\n",
    "print('prédiction: ', clf.predict(VW))\n",
    "print('proba de perdre: ', clf.predict_proba(VW)[0][0])\n",
    "print('proba de gagner: ', clf.predict_proba(VW)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898e5d2-ff9a-4ad8-b241-e03128282559",
   "metadata": {},
   "source": [
    "##### On trouve alors un résultat très catégorique: on est sûr de perdre. Cependant, ce résultat n'est pas fiable à cause de la \"faible\" quantité d'individus d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a102e-8635-43a3-97da-65202696b5d0",
   "metadata": {},
   "source": [
    "##### Il nous paraît alors plus pertinent d'effectuer un arbre de décision puisque c'est un algorithme de ML qui crée un arbre de décision en séparant au mieux les données d'entraînement selon le booléen à prédire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ada37bc-8490-4093-bd64-9a54ef45b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les résultats suivants ne sont pas consistants suivant la séparation entrainement/vérif\n",
      "prédiction:  [0]\n",
      "proba de perdre:  1.0\n",
      "proba de gagner:  0.0\n"
     ]
    }
   ],
   "source": [
    "#arbre de décision\n",
    "clf = tree.DecisionTreeClassifier(max_depth=6)\n",
    "clf = clf.fit(X_scaled, Y)\n",
    "print('Les résultats suivants ne sont pas consistants suivant la séparation entrainement/vérif')\n",
    "print('prédiction: ', clf.predict(VW))\n",
    "print('proba de perdre: ', clf.predict_proba(VW)[0][0])\n",
    "print('proba de gagner: ', clf.predict_proba(VW)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994dac2-4807-4dc0-bdd2-4706dfcaff55",
   "metadata": {},
   "source": [
    "##### Modéliser notre problématique avec une forêt aléatoire nous paraît encore plus pertinent puisque cet algorithme de ML crée plusieurs arbres aléatoires avec différents échantillons d'entraînement, puis compte les résultats des différents arbres lors d'une prédiction. Cela permet donc de réduire l'impact de l'aléatoire qui nous posait problème avec l'arbre de décision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a121699e-c18c-49e3-9080-c2792b8f5bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prédiction:  [1]\n",
      "proba de perdre:  0.3252828134628465\n",
      "proba de gagner:  0.6747171865371532\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "clf = RandomForestClassifier(max_depth=2)\n",
    "clf.fit(X_scaled, Y)\n",
    "print('prédiction: ', clf.predict(VW))\n",
    "print('proba de perdre: ', clf.predict_proba(VW)[0][0])\n",
    "print('proba de gagner: ', clf.predict_proba(VW)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1eef98-131d-469a-b35a-d900d0c38638",
   "metadata": {},
   "source": [
    "##### On trouve alors une probabilité plutôt stable de gagner le match qui est entre 0,60 et 0,70 et donc entre 0,30 et 0,40 pour la probabilité de perdre le match. Dans notre exemple, on a donc intérêt à mettre Victor Wembanyama sur le terrain pour gagner un match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56fcd9-d367-46d2-96f0-f534b2ba41b5",
   "metadata": {},
   "source": [
    "##### On a fait le choix de ne pas rajouter davantage de données afin de montrer les limites de certaines modélisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc976c6-2448-4e5a-9f2c-cbd70c0e42c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
